{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF Loading and Preprocessing Techniques with LangChain\n",
    "\n",
    "## Overview\n",
    "This notebook provides a comprehensive introduction to document loading techniques using PyMuPDF and LangChain, focusing on extracting and preparing PDF documents for advanced information retrieval and analysis. The guide demonstrates robust methods for discovering, loading, and preprocessing PDF files from various sources.\n",
    "\n",
    "## Key Features:\n",
    "- Recursive PDF document discovery\n",
    "- Flexible directory-based document loading\n",
    "- Metadata preservation during extraction\n",
    "- Preparation for semantic vectorization\n",
    "- Scalable document processing approach\n",
    "\n",
    "## Technologies Used:\n",
    "- PyMuPDF\n",
    "- LangChain Document Loaders\n",
    "- File system traversal\n",
    "- Metadata extraction\n",
    "- Document preprocessing utilities\n",
    "\n",
    "## Use Cases:\n",
    "- Medical document intelligence\n",
    "- Legal document analysis\n",
    "- Financial record processing\n",
    "- Academic research document management\n",
    "- Compliance and regulatory document review\n",
    "\n",
    "## Activities Covered in This Notebook\n",
    "\n",
    "1. **PDF Document Discovery**  \n",
    "    - Implementing recursive directory scanning\n",
    "    - Identifying and filtering PDF files\n",
    "    - Creating a comprehensive document collection\n",
    "\n",
    "2. **PDF Text Extraction**  \n",
    "    - Utilizing PyMuPDF for precise text extraction\n",
    "    - Preserving document structure and formatting\n",
    "    - Handling complex PDF layouts and encodings\n",
    "\n",
    "3. **Metadata Management**  \n",
    "    - Extracting document-level metadata\n",
    "    - Preserving source information\n",
    "    - Preparing metadata for future processing stages\n",
    "\n",
    "4. **Document Loading Strategies**  \n",
    "    - Exploring different loading approaches\n",
    "    - Managing large document collections\n",
    "    - Implementing efficient loading mechanisms\n",
    "\n",
    "5. **Error Handling and Robustness**  \n",
    "    - Implementing basic error management\n",
    "    - Handling potential loading exceptions\n",
    "    - Ensuring consistent document extraction\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "This notebook provides a foundational understanding of PDF document loading techniques. In upcoming notebooks, we will explore advanced topics, including:\n",
    "\n",
    "- **Text Splitting and Chunking**: Breaking documents into semantic chunks\n",
    "- **Embedding Generation**: Converting text to numerical representations\n",
    "- **Vector Store Creation**: Indexing documents for semantic search\n",
    "- **Advanced Retrieval Techniques**: Implementing sophisticated information retrieval methods\n",
    "- **Metadata Enrichment**: Adding contextual information to document chunks\n",
    "\n",
    "\n",
    "Stay tuned for more detailed discussions and hands-on examples!\n",
    "\n",
    "> **Sidenote:** Ensure that you have selected the kernel as the conda environment named `langchain`, as instructed in Lab Guide 1. This is crucial for running the code in this notebook without any issues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain_community\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import warnings\n",
    "import tiktoken\n",
    "import faiss\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Document Loading Libraries\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Method: `load_pdf_documents`\n",
    "\n",
    "The `load_pdf_documents` method is designed to recursively load PDF documents from a specified directory. It identifies all PDF files within the directory and its subdirectories, extracts their content, and returns a list of loaded documents.\n",
    "\n",
    "#### **Function Signature**\n",
    "```python\n",
    "def load_pdf_documents(directory):\n",
    "```\n",
    "\n",
    "#### **Parameters**\n",
    "- `directory` (str): The path to the directory containing PDF files. This can include subdirectories, as the method performs recursive scanning.\n",
    "\n",
    "#### **Returns**\n",
    "- `list`: A list of loaded documents, where each document represents the content extracted from a PDF file.\n",
    "\n",
    "#### **Workflow**\n",
    "1. **Recursive File Discovery**:\n",
    "    - The method uses `os.walk()` to traverse the directory and its subdirectories.\n",
    "    - It identifies all files with a `.pdf` extension and stores their paths in a list called `pdfs`.\n",
    "\n",
    "2. **PDF Loading**:\n",
    "    - For each PDF file in the `pdfs` list, the method initializes a `PyMuPDFLoader` instance.\n",
    "    - The loader extracts the content of the PDF and appends it to the `docs` list.\n",
    "\n",
    "3. **Return Loaded Documents**:\n",
    "    - The method returns the `docs` list, which contains the content of all discovered and loaded PDF files.\n",
    "\n",
    "#### **Example Usage**\n",
    "```python\n",
    "# Load all PDF documents from the specified directory\n",
    "documents = load_pdf_documents(\"../dataset/health_docs\")\n",
    "\n",
    "# Check the number of loaded documents\n",
    "print(f\"Total documents loaded: {len(documents)}\")\n",
    "```\n",
    "\n",
    "#### **Key Features**\n",
    "- **Recursive Scanning**: Ensures all PDF files in nested directories are discovered.\n",
    "- **Flexible Loading**: Uses `PyMuPDFLoader` for robust PDF content extraction.\n",
    "- **Scalability**: Handles large collections of PDF files efficiently.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf_documents(directory):\n",
    "    \"\"\"\n",
    "    Load PDF documents from a specified directory.\n",
    "    \n",
    "    Args:\n",
    "        directory (str): Path to the directory containing PDF files\n",
    "    \n",
    "    Returns:\n",
    "        list: List of loaded documents\n",
    "    \"\"\"\n",
    "    pdfs = []\n",
    "    docs = []\n",
    "    \n",
    "    # Find all PDF files in the specified directory\n",
    "    for root, _, files in os.walk(directory):\n",
    "        pdfs.extend([os.path.join(root, file) for file in files if file.endswith(\".pdf\")])\n",
    "    \n",
    "    # Load each PDF document\n",
    "    for pdf in pdfs:\n",
    "        loader = PyMuPDFLoader(pdf)\n",
    "        docs.extend(loader.load())\n",
    "    \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `chunk_documents`\n",
    "\n",
    "The `chunk_documents` method is designed to split large documents into smaller, manageable chunks. This is particularly useful for processing lengthy documents in tasks such as semantic search, embedding generation, and information retrieval.\n",
    "\n",
    "#### **Function Signature**\n",
    "```python\n",
    "def chunk_documents(docs, chunk_size=1000, chunk_overlap=100):\n",
    "```\n",
    "\n",
    "#### **Parameters**\n",
    "- `docs` (list): A list of documents to be split into chunks. Each document is expected to have a `page_content` attribute containing the text.\n",
    "- `chunk_size` (int): The maximum size of each chunk in terms of characters. Default is 1000.\n",
    "- `chunk_overlap` (int): The number of overlapping characters between consecutive chunks. Default is 100.\n",
    "\n",
    "#### **Returns**\n",
    "- `list`: A list of document chunks, where each chunk is a smaller portion of the original document.\n",
    "\n",
    "#### **Workflow**\n",
    "1. **Initialize Text Splitter**:\n",
    "    - The method uses the `RecursiveCharacterTextSplitter` from LangChain to handle the splitting process.\n",
    "    - The splitter is configured with the specified `chunk_size` and `chunk_overlap`.\n",
    "\n",
    "2. **Split Documents**:\n",
    "    - The `split_documents` method of the text splitter is applied to the input `docs`.\n",
    "    - This generates a list of smaller chunks, ensuring that no chunk exceeds the specified size and that overlapping content is preserved.\n",
    "\n",
    "3. **Return Chunks**:\n",
    "    - The method returns the list of chunks, which can be used for downstream tasks such as embedding generation or vector store creation.\n",
    "\n",
    "#### **Example Usage**\n",
    "```python\n",
    "# Split documents into chunks\n",
    "chunks = chunk_documents(docs, chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "# Check the number of chunks created\n",
    "print(f\"Total document chunks: {len(chunks)}\")\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_documents(docs, chunk_size=1000, chunk_overlap=100):\n",
    "    \"\"\"\n",
    "    Split documents into smaller chunks.\n",
    "    \n",
    "    Args:\n",
    "        docs (list): List of documents to chunk\n",
    "        chunk_size (int): Size of each document chunk\n",
    "        chunk_overlap (int): Overlap between chunks\n",
    "    \n",
    "    Returns:\n",
    "        list: List of document chunks\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, \n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    return text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `print_retrieved_docs`\n",
    "\n",
    "The `print_retrieved_docs` method is designed to display retrieved documents in a clean and readable format. This is particularly useful for reviewing the results of a document retrieval process, such as those obtained from a vector store or semantic search.\n",
    "\n",
    "#### **Function Signature**\n",
    "```python\n",
    "def print_retrieved_docs(retrieved_docs, max_length=500):\n",
    "```\n",
    "\n",
    "#### **Parameters**\n",
    "- `retrieved_docs` (list): A list of retrieved documents. Each document is expected to have metadata and content attributes.\n",
    "- `max_length` (int): The maximum length of the document content to display. If the content exceeds this length, it will be truncated. Default is 500.\n",
    "\n",
    "#### **Workflow**\n",
    "1. **Print Summary**:\n",
    "    - The method begins by printing the total number of retrieved documents and a separator line for clarity.\n",
    "\n",
    "2. **Iterate Through Documents**:\n",
    "    - For each document in the `retrieved_docs` list, the method:\n",
    "        - Displays the document's index.\n",
    "        - Prints the `score` and `source` metadata, if available. If not, it defaults to \"N/A\" or \"Unknown\".\n",
    "\n",
    "3. **Truncate Content**:\n",
    "    - If the document's `page_content` exceeds the `max_length`, it truncates the content and appends a `[truncated]` note.\n",
    "\n",
    "4. **Display Content**:\n",
    "    - The truncated or full content of the document is printed, followed by a separator line for readability.\n",
    "\n",
    "#### **Example Usage**\n",
    "```python\n",
    "# Example list of retrieved documents\n",
    "retrieved_docs = [\n",
    "    {\"metadata\": {\"score\": 0.95, \"source\": \"doc1.pdf\"}, \"page_content\": \"This is the content of document 1.\"},\n",
    "    {\"metadata\": {\"score\": 0.85, \"source\": \"doc2.pdf\"}, \"page_content\": \"This is the content of document 2.\"}\n",
    "]\n",
    "\n",
    "# Print the retrieved documents\n",
    "print_retrieved_docs(retrieved_docs)\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_retrieved_docs(retrieved_docs, max_length=500):\n",
    "    \"\"\"\n",
    "    Print retrieved documents in a clean, readable format.\n",
    "    \n",
    "    Args:\n",
    "        retrieved_docs (list): List of retrieved documents\n",
    "        max_length (int): Maximum length of content to display\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Retrieved Documents ---\")\n",
    "    print(f\"Total documents retrieved: {len(retrieved_docs)}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for i, doc in enumerate(retrieved_docs, 1):\n",
    "        print(f\"\\nDocument {i}:\")\n",
    "        print(f\"Score: {doc.metadata.get('score', 'N/A')}\")\n",
    "        print(f\"Source: {doc.metadata.get('source', 'Unknown')}\")\n",
    "        \n",
    "        # Truncate content if it's too long\n",
    "        content = doc.page_content\n",
    "        if len(content) > max_length:\n",
    "            content = content[:max_length] + \"... [truncated]\"\n",
    "        \n",
    "        print(\"\\nContent:\")\n",
    "        print(content)\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method `create_vector_store`\n",
    "\n",
    "The `create_vector_store` function is designed to create a vector store from document chunks. This vector store is used for efficient semantic search and retrieval tasks by embedding the document chunks into a vector space.\n",
    "\n",
    "#### **Function Signature**\n",
    "```python\n",
    "def create_vector_store(chunks, embedding_model='nomic-embed-text', base_url='http://localhost:11434'):\n",
    "```\n",
    "\n",
    "#### **Parameters**\n",
    "- `chunks` (list): A list of document chunks to be embedded and stored in the vector store.\n",
    "- `embedding_model` (str): The name of the embedding model to be used for generating vector embeddings. Default is `'nomic-embed-text'`.\n",
    "- `base_url` (str): The base URL for the embedding model service. Default is `'http://localhost:11434'`.\n",
    "\n",
    "#### **Returns**\n",
    "- `FAISS`: A FAISS-based vector store containing the embedded document chunks.\n",
    "\n",
    "#### **Workflow**\n",
    "1. **Initialize Embeddings**:\n",
    "    - The function initializes an `OllamaEmbeddings` instance using the specified `embedding_model` and `base_url`.\n",
    "    - A sample query (`\"Hello World\"`) is embedded to determine the vector size.\n",
    "\n",
    "2. **Create FAISS Index**:\n",
    "    - A FAISS index is created using `faiss.IndexFlatL2` with the determined vector size.\n",
    "    - The FAISS index is wrapped in a `FAISS` object, which includes:\n",
    "        - The embedding function.\n",
    "        - An in-memory document store (`InMemoryDocstore`).\n",
    "        - A mapping of index IDs to document store IDs.\n",
    "\n",
    "3. **Add Documents to Vector Store**:\n",
    "    - The document chunks are embedded and added to the FAISS vector store.\n",
    "\n",
    "4. **Return Vector Store**:\n",
    "    - The function returns the FAISS vector store, which can be used for semantic search and retrieval.\n",
    "\n",
    "#### **Example Usage**\n",
    "```python\n",
    "# Create a vector store from document chunks\n",
    "vector_store = create_vector_store(chunks)\n",
    "\n",
    "# Example query\n",
    "question = \"What are the benefits of regular exercise?\"\n",
    "retrieved_docs = vector_store.search(query=question, k=5, search_type=\"similarity\")\n",
    "\n",
    "# Display retrieved documents\n",
    "print_retrieved_docs(retrieved_docs)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_store(chunks, embedding_model='nomic-embed-text', base_url='http://localhost:11434'):\n",
    "    \"\"\"\n",
    "    Create a vector store from document chunks.\n",
    "    \n",
    "    Args:\n",
    "        chunks (list): List of document chunks\n",
    "        embedding_model (str): Name of the embedding model\n",
    "        base_url (str): Base URL for Ollama embeddings\n",
    "    \n",
    "    Returns:\n",
    "        FAISS: Vector store with embedded documents\n",
    "    \"\"\"\n",
    "    # Initialize embeddings\n",
    "    embeddings = OllamaEmbeddings(model=embedding_model, base_url=base_url)\n",
    "    \n",
    "    # Create vector embedding\n",
    "    vector = embeddings.embed_query(\"Hello World\")\n",
    "    \n",
    "    # Create FAISS index\n",
    "    index = faiss.IndexFlatL2(len(vector))\n",
    "    vector_store = FAISS(\n",
    "        embedding_function=embeddings,\n",
    "        index=index,\n",
    "        docstore=InMemoryDocstore(),\n",
    "        index_to_docstore_id={},\n",
    "    )\n",
    "    \n",
    "    # Add documents to vector store\n",
    "    vector_store.add_documents(documents=chunks)\n",
    "    \n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Pages loaded: 38\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Main function to orchestrate document processing and vector store creation.\n",
    "    \"\"\"\n",
    "    # Suppress warnings (optional)\n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "    # Step 1 : Load PDF documents\n",
    "    docs = load_pdf_documents(\"../dataset/health_docs\")\n",
    "    \n",
    "    # Optional: Check document count and content\n",
    "    print(f\"Total Pages loaded: {len(docs)}\")\n",
    "    \n",
    "   \n",
    "    \n",
    "    # Step 2 : Chunk documents\n",
    "    # chunks = chunk_documents(docs)\n",
    "    # print(f\"Total document chunks: {len(chunks)}\")\n",
    "    \n",
    "    # Optional: Tokenization check\n",
    "    # encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "    # token_lengths = [len(encoding.encode(chunk.page_content)) for chunk in chunks[:3]]\n",
    "    # print(f\"Token lengths of first 3 chunks: {token_lengths}\")\n",
    "    \n",
    "    # Step 3 : Create vector store, Embeddings\n",
    "    # vector_store = create_vector_store(chunks)\n",
    "    \n",
    "    # Example retrieval\n",
    "    # question = \"What nutritional supplements support muscle protein synthesis?\"\n",
    "    # retrieved_docs = vector_store.search(query=question, k=5, search_type=\"similarity\")\n",
    "\n",
    "    # print_retrieved_docs(retrieved_docs)\n",
    "    \n",
    "    # # Optional: Save vector store\n",
    "    # db_name = \"../health_docs\"\n",
    "    # vector_store.save_local(db_name)\n",
    "    \n",
    "    # Step 4 : Load the vector store from the saved location\n",
    "    \n",
    "    # Step 5 : Configure retriever with search parameters\n",
    "    \n",
    "    # Step 6 : Build the retrieval chain\n",
    "     \n",
    "    # Step 7 : RAG-based retrieval and generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
