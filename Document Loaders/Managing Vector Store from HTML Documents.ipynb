{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Managing Vector Store from HTML Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents loaded: 3\n",
      "Total document chunks: 64\n",
      "\n",
      "--- Retrieved Documents ---\n",
      "Total documents retrieved: 5\n",
      "--------------------------------------------------\n",
      "\n",
      "Document 1:\n",
      "Score: N/A\n",
      "Source: https://python.langchain.com/docs/introduction/\n",
      "\n",
      "Content:\n",
      "LangChain\n",
      " is a framework for developing applications powered by large language models (LLMs).\n",
      "\n",
      "\n",
      "LangChain simplifies every stage of the LLM application lifecycle:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Development\n",
      ": Build your applications using LangChain's open-source \n",
      "components\n",
      " and \n",
      "third-party integrations\n",
      ".\n",
      "Use \n",
      "LangGraph\n",
      " to build stateful agents with first-class streaming and human-in-the-loop support.\n",
      "\n",
      "\n",
      "Productionization\n",
      ": Use \n",
      "LangSmith\n",
      " to inspect, monitor and evaluate your applications, so that you can continuously o... [truncated]\n",
      "--------------------------------------------------\n",
      "\n",
      "Document 2:\n",
      "Score: N/A\n",
      "Source: https://python.langchain.com/docs/introduction/\n",
      "\n",
      "Content:\n",
      "Architecture\n",
      " page.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "langchain-core\n",
      ": Base abstractions for chat models and other components.\n",
      "\n",
      "\n",
      "Integration packages\n",
      " (e.g. \n",
      "langchain-openai\n",
      ", \n",
      "langchain-anthropic\n",
      ", etc.): Important integrations have been split into lightweight packages that are co-maintained by the LangChain team and the integration developers.\n",
      "\n",
      "\n",
      "langchain\n",
      ": Chains, agents, and retrieval strategies that make up an application's cognitive architecture.\n",
      "\n",
      "\n",
      "langchain-community\n",
      ": Third-party integrations that are community main... [truncated]\n",
      "--------------------------------------------------\n",
      "\n",
      "Document 3:\n",
      "Score: N/A\n",
      "Source: https://python.langchain.com/docs/concepts/chat_models/\n",
      "\n",
      "Content:\n",
      "LangChain supports two message formats to interact with chat models:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LangChain Message Format\n",
      ": LangChain's own message format, which is used by default and is used internally by LangChain.\n",
      "\n",
      "\n",
      "OpenAI's Message Format\n",
      ": OpenAI's message format.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Standard parameters\n",
      "​\n",
      "\n",
      "\n",
      "Many chat models have standardized parameters that can be used to configure the model:\n",
      "--------------------------------------------------\n",
      "\n",
      "Document 4:\n",
      "Score: N/A\n",
      "Source: https://python.langchain.com/docs/introduction/\n",
      "\n",
      "Content:\n",
      "Introduction | 🦜️🔗 LangChain\n",
      "--------------------------------------------------\n",
      "\n",
      "Document 5:\n",
      "Score: N/A\n",
      "Source: https://python.langchain.com/docs/introduction/\n",
      "\n",
      "Content:\n",
      "Embedding models\n",
      "Evaluation\n",
      "Example selectors\n",
      "Few-shot prompting\n",
      "Conceptual guide\n",
      "Key-value stores\n",
      "LangChain Expression Language (LCEL)\n",
      "Messages\n",
      "Multimodality\n",
      "Output parsers\n",
      "Prompt Templates\n",
      "Retrieval augmented generation (RAG)\n",
      "Retrieval\n",
      "Retrievers\n",
      "Runnable interface\n",
      "Streaming\n",
      "Structured outputs\n",
      "Testing\n",
      "String-in, string-out llms\n",
      "Text splitters\n",
      "Tokens\n",
      "Tool calling\n",
      "Tools\n",
      "Tracing\n",
      "Vector stores\n",
      "Why LangChain?\n",
      "Ecosystem\n",
      "🦜🛠️ LangSmith\n",
      "🦜🕸️ LangGraph\n",
      "Versions\n",
      "v0.3\n",
      "v0.2\n",
      "Pydantic compatibility\n",
      "Migrating ... [truncated]\n",
      "--------------------------------------------------\n",
      "Vector store saved to ../langchain_docs.\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import warnings\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.docstore.document import Document  # Import Document class\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "import faiss\n",
    "\n",
    "def load_html_documents(urls):\n",
    "    \"\"\"\n",
    "    Load HTML documents from a list of URLs.\n",
    "    \n",
    "    Args:\n",
    "        urls (list): List of URLs to fetch HTML content from\n",
    "    \n",
    "    Returns:\n",
    "        list: List of Document objects\n",
    "    \"\"\"\n",
    "    docs = []\n",
    "    \n",
    "    for url in urls:\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Extract text content from the HTML\n",
    "            text = soup.get_text(separator=\"\\n\")\n",
    "            # Convert to Document object\n",
    "            doc = Document(page_content=text, metadata={\"source\": url})\n",
    "            docs.append(doc)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load {url}: {e}\")\n",
    "    \n",
    "    return docs\n",
    "\n",
    "def chunk_documents(docs, chunk_size=1000, chunk_overlap=100):\n",
    "    \"\"\"\n",
    "    Split documents into smaller chunks.\n",
    "    \n",
    "    Args:\n",
    "        docs (list): List of Document objects to chunk\n",
    "        chunk_size (int): Size of each document chunk\n",
    "        chunk_overlap (int): Overlap between chunks\n",
    "    \n",
    "    Returns:\n",
    "        list: List of Document chunks\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, \n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    return text_splitter.split_documents(docs)\n",
    "\n",
    "def create_vector_store(chunks, embedding_model='nomic-embed-text', base_url='http://localhost:11434'):\n",
    "    \"\"\"\n",
    "    Create a vector store from document chunks.\n",
    "    \n",
    "    Args:\n",
    "        chunks (list): List of document chunks\n",
    "        embedding_model (str): Name of the embedding model\n",
    "        base_url (str): Base URL for Ollama embeddings\n",
    "    \n",
    "    Returns:\n",
    "        FAISS: Vector store with embedded documents\n",
    "    \"\"\"\n",
    "    # Initialize embeddings\n",
    "    embeddings = OllamaEmbeddings(model=embedding_model, base_url=base_url)\n",
    "    \n",
    "    # Create vector embedding\n",
    "    vector = embeddings.embed_query(\"Sample Text\")\n",
    "    \n",
    "    # Create FAISS index\n",
    "    index = faiss.IndexFlatL2(len(vector))\n",
    "    vector_store = FAISS(\n",
    "        embedding_function=embeddings,\n",
    "        index=index,\n",
    "        docstore=InMemoryDocstore(),\n",
    "        index_to_docstore_id={},\n",
    "    )\n",
    "    \n",
    "    # Add documents to vector store\n",
    "    vector_store.add_documents(documents=chunks)\n",
    "    \n",
    "    return vector_store\n",
    "\n",
    "def print_retrieved_docs(retrieved_docs, max_length=500):\n",
    "    \"\"\"\n",
    "    Print retrieved documents in a clean, readable format.\n",
    "    \n",
    "    Args:\n",
    "        retrieved_docs (list): List of retrieved documents\n",
    "        max_length (int): Maximum length of content to display\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Retrieved Documents ---\")\n",
    "    print(f\"Total documents retrieved: {len(retrieved_docs)}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for i, doc in enumerate(retrieved_docs, 1):\n",
    "        print(f\"\\nDocument {i}:\")\n",
    "        print(f\"Score: {doc.metadata.get('score', 'N/A')}\")\n",
    "        print(f\"Source: {doc.metadata.get('source', 'Unknown')}\")\n",
    "        \n",
    "        # Truncate content if it's too long\n",
    "        content = doc.page_content\n",
    "        if len(content) > max_length:\n",
    "            content = content[:max_length] + \"... [truncated]\"\n",
    "        \n",
    "        print(\"\\nContent:\")\n",
    "        print(content)\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Main function to orchestrate document processing and vector store creation.\n",
    "    \"\"\"\n",
    "    # Suppress warnings (optional)\n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "    # List of URLs to process\n",
    "    urls = [\n",
    "        \"https://python.langchain.com/docs/introduction/\",\n",
    "        \"https://python.langchain.com/docs/concepts/chat_models/\",\n",
    "        \"https://python.langchain.com/docs/concepts/vectorstores/\"\n",
    "    ]\n",
    "    \n",
    "    # Load HTML documents\n",
    "    docs = load_html_documents(urls)\n",
    "    print(f\"Total documents loaded: {len(docs)}\")\n",
    "    \n",
    "    # Chunk documents\n",
    "    chunks = chunk_documents(docs)\n",
    "    print(f\"Total document chunks: {len(chunks)}\")\n",
    "    \n",
    "    # Create vector store\n",
    "    # vector_store = create_vector_store(chunks)\n",
    "    \n",
    "    # Example retrieval\n",
    "    # question = \"Which usecases  LangChain is used for?\"\n",
    "    # try:\n",
    "    #     retrieved_docs = vector_store.similarity_search(query=question, k=5)\n",
    "    #     if not retrieved_docs:\n",
    "    #         print(\"No documents retrieved.\")\n",
    "    #     else:\n",
    "    #         print_retrieved_docs(retrieved_docs)\n",
    "    # except Exception as e:\n",
    "    #     print(f\"Error during similarity search: {e}\")\n",
    "    \n",
    "    # Optional: Save vector store\n",
    "    # db_name = \"../langchain_docs\"\n",
    "    # try:\n",
    "    #     vector_store.save_local(db_name)\n",
    "    #     print(f\"Vector store saved to {db_name}.\")\n",
    "    # except Exception as e:\n",
    "    #     print(f\"Failed to save vector store: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
