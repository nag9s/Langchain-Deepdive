{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF Loading and Preprocessing Techniques with LangChain\n",
    "\n",
    "## Overview\n",
    "This notebook provides a comprehensive introduction to document loading techniques using PyMuPDF and LangChain, focusing on extracting and preparing PDF documents for advanced information retrieval and analysis. The guide demonstrates robust methods for discovering, loading, and preprocessing PDF files from various sources.\n",
    "\n",
    "## Key Features:\n",
    "- Recursive PDF document discovery\n",
    "- Flexible directory-based document loading\n",
    "- Metadata preservation during extraction\n",
    "- Preparation for semantic vectorization\n",
    "- Scalable document processing approach\n",
    "\n",
    "## Technologies Used:\n",
    "- PyMuPDF\n",
    "- LangChain Document Loaders\n",
    "- File system traversal\n",
    "- Metadata extraction\n",
    "- Document preprocessing utilities\n",
    "\n",
    "## Use Cases:\n",
    "- Medical document intelligence\n",
    "- Legal document analysis\n",
    "- Financial record processing\n",
    "- Academic research document management\n",
    "- Compliance and regulatory document review\n",
    "\n",
    "## Activities Covered in This Notebook\n",
    "\n",
    "1. **PDF Document Discovery**  \n",
    "    - Implementing recursive directory scanning\n",
    "    - Identifying and filtering PDF files\n",
    "    - Creating a comprehensive document collection\n",
    "\n",
    "2. **PDF Text Extraction**  \n",
    "    - Utilizing PyMuPDF for precise text extraction\n",
    "    - Preserving document structure and formatting\n",
    "    - Handling complex PDF layouts and encodings\n",
    "\n",
    "3. **Metadata Management**  \n",
    "    - Extracting document-level metadata\n",
    "    - Preserving source information\n",
    "    - Preparing metadata for future processing stages\n",
    "\n",
    "4. **Document Loading Strategies**  \n",
    "    - Exploring different loading approaches\n",
    "    - Managing large document collections\n",
    "    - Implementing efficient loading mechanisms\n",
    "\n",
    "5. **Error Handling and Robustness**  \n",
    "    - Implementing basic error management\n",
    "    - Handling potential loading exceptions\n",
    "    - Ensuring consistent document extraction\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "This notebook provides a foundational understanding of PDF document loading techniques. In upcoming notebooks, we will explore advanced topics, including:\n",
    "\n",
    "- **Text Splitting and Chunking**: Breaking documents into semantic chunks\n",
    "- **Embedding Generation**: Converting text to numerical representations\n",
    "- **Vector Store Creation**: Indexing documents for semantic search\n",
    "- **Advanced Retrieval Techniques**: Implementing sophisticated information retrieval methods\n",
    "- **Metadata Enrichment**: Adding contextual information to document chunks\n",
    "\n",
    "\n",
    "Stay tuned for more detailed discussions and hands-on examples!\n",
    "\n",
    "> **Sidenote:** Ensure that you have selected the kernel as the conda environment named `langchain`, as instructed in Lab Guide 1. This is crucial for running the code in this notebook without any issues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents loaded: 38\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import warnings\n",
    "import tiktoken\n",
    "import faiss\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Document Loading Libraries\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "\n",
    "def load_pdf_documents(directory):\n",
    "    \"\"\"\n",
    "    Load PDF documents from a specified directory.\n",
    "    \n",
    "    Args:\n",
    "        directory (str): Path to the directory containing PDF files\n",
    "    \n",
    "    Returns:\n",
    "        list: List of loaded documents\n",
    "    \"\"\"\n",
    "    pdfs = []\n",
    "    docs = []\n",
    "    \n",
    "    # Find all PDF files in the specified directory\n",
    "    for root, _, files in os.walk(directory):\n",
    "        pdfs.extend([os.path.join(root, file) for file in files if file.endswith(\".pdf\")])\n",
    "    \n",
    "    # Load each PDF document\n",
    "    for pdf in pdfs:\n",
    "        loader = PyMuPDFLoader(pdf)\n",
    "        docs.extend(loader.load())\n",
    "    \n",
    "    return docs\n",
    "\n",
    "def chunk_documents(docs, chunk_size=1000, chunk_overlap=100):\n",
    "    \"\"\"\n",
    "    Split documents into smaller chunks.\n",
    "    \n",
    "    Args:\n",
    "        docs (list): List of documents to chunk\n",
    "        chunk_size (int): Size of each document chunk\n",
    "        chunk_overlap (int): Overlap between chunks\n",
    "    \n",
    "    Returns:\n",
    "        list: List of document chunks\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, \n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    return text_splitter.split_documents(docs)\n",
    "\n",
    "def create_vector_store(chunks, embedding_model='nomic-embed-text', base_url='http://localhost:11434'):\n",
    "    \"\"\"\n",
    "    Create a vector store from document chunks.\n",
    "    \n",
    "    Args:\n",
    "        chunks (list): List of document chunks\n",
    "        embedding_model (str): Name of the embedding model\n",
    "        base_url (str): Base URL for Ollama embeddings\n",
    "    \n",
    "    Returns:\n",
    "        FAISS: Vector store with embedded documents\n",
    "    \"\"\"\n",
    "    # Initialize embeddings\n",
    "    embeddings = OllamaEmbeddings(model=embedding_model, base_url=base_url)\n",
    "    \n",
    "    # Create vector embedding\n",
    "    vector = embeddings.embed_query(\"Hello World\")\n",
    "    \n",
    "    # Create FAISS index\n",
    "    index = faiss.IndexFlatL2(len(vector))\n",
    "    vector_store = FAISS(\n",
    "        embedding_function=embeddings,\n",
    "        index=index,\n",
    "        docstore=InMemoryDocstore(),\n",
    "        index_to_docstore_id={},\n",
    "    )\n",
    "    \n",
    "    # Add documents to vector store\n",
    "    vector_store.add_documents(documents=chunks)\n",
    "    \n",
    "    return vector_store\n",
    "def print_retrieved_docs(retrieved_docs, max_length=500):\n",
    "    \"\"\"\n",
    "    Print retrieved documents in a clean, readable format.\n",
    "    \n",
    "    Args:\n",
    "        retrieved_docs (list): List of retrieved documents\n",
    "        max_length (int): Maximum length of content to display\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Retrieved Documents ---\")\n",
    "    print(f\"Total documents retrieved: {len(retrieved_docs)}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for i, doc in enumerate(retrieved_docs, 1):\n",
    "        print(f\"\\nDocument {i}:\")\n",
    "        print(f\"Score: {doc.metadata.get('score', 'N/A')}\")\n",
    "        print(f\"Source: {doc.metadata.get('source', 'Unknown')}\")\n",
    "        \n",
    "        # Truncate content if it's too long\n",
    "        content = doc.page_content\n",
    "        if len(content) > max_length:\n",
    "            content = content[:max_length] + \"... [truncated]\"\n",
    "        \n",
    "        print(\"\\nContent:\")\n",
    "        print(content)\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Main function to orchestrate document processing and vector store creation.\n",
    "    \"\"\"\n",
    "    # Suppress warnings (optional)\n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "    # Load PDF documents\n",
    "    docs = load_pdf_documents(\"../dataset/health_docs\")\n",
    "    \n",
    "    # Optional: Check document count and content\n",
    "    print(f\"Total Pages loaded: {len(docs)}\")\n",
    "    \n",
    "   \n",
    "    \n",
    "    # Chunk documents\n",
    "    # chunks = chunk_documents(docs)\n",
    "    # print(f\"Total document chunks: {len(chunks)}\")\n",
    "    \n",
    "    # Optional: Tokenization check\n",
    "    # encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "    # token_lengths = [len(encoding.encode(chunk.page_content)) for chunk in chunks[:3]]\n",
    "    # print(f\"Token lengths of first 3 chunks: {token_lengths}\")\n",
    "    \n",
    "    # Create vector store\n",
    "    # vector_store = create_vector_store(chunks)\n",
    "    \n",
    "    # Example retrieval\n",
    "    # question = \"What nutritional supplements support muscle protein synthesis?\"\n",
    "    # retrieved_docs = vector_store.search(query=question, k=5, search_type=\"similarity\")\n",
    "\n",
    "    # print_retrieved_docs(retrieved_docs)\n",
    "    \n",
    "    # # Optional: Save vector store\n",
    "    # db_name = \"../health_docs\"\n",
    "    # vector_store.save_local(db_name)\n",
    "    \n",
    "    # Load the vector store from the saved location\n",
    "    \n",
    "    # Configure retriever with search parameters\n",
    "    \n",
    "    # Build the retrieval chain\n",
    "     \n",
    "    # RAG-based retrieval and generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
