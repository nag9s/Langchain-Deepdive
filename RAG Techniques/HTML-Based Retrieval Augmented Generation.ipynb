{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HTML-Based Retrieval Augmented Generation (RAG): Intelligent Document Querying\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates the implementation of a Retrieval Augmented Generation (RAG) pipeline using PDF documents, showcasing how to combine document retrieval with intelligent language model responses. The guide provides a practical approach to creating context-aware, document-grounded question-answering systems.\n",
    "\n",
    "## Key Features:\n",
    "- Semantic document retrieval\n",
    "- Context-aware response generation\n",
    "- PDF-based knowledge querying\n",
    "- Intelligent information extraction\n",
    "- Flexible RAG pipeline construction\n",
    "\n",
    "## Technologies Used:\n",
    "- Ollama Language Models\n",
    "- FAISS Vector Store\n",
    "- Semantic Retrieval\n",
    "- Prompt Engineering\n",
    "- Context-Based Generation\n",
    "\n",
    "## Use Cases:\n",
    "- Intelligent document querying\n",
    "- Medical research information extraction\n",
    "- Technical documentation analysis\n",
    "- Contextual question answering\n",
    "- Knowledge base exploration\n",
    "\n",
    "## Activities Covered in This Notebook\n",
    "\n",
    "1. **Vector Store Retrieval**  \n",
    "    - Loading pre-indexed document vectors\n",
    "    - Configuring semantic search parameters\n",
    "    - Retrieving most relevant document chunks\n",
    "\n",
    "2. **RAG Pipeline Construction**  \n",
    "    - Designing context-aware prompt templates\n",
    "    - Integrating retriever with language model\n",
    "    - Creating flexible generation pipeline\n",
    "\n",
    "3. **Intelligent Querying**  \n",
    "    - Performing semantic search\n",
    "    - Retrieving contextually relevant documents\n",
    "    - Generating informed responses\n",
    "\n",
    "4. **Response Generation**  \n",
    "    - Using retrieved context to ground LLM responses\n",
    "    - Implementing fallback mechanisms\n",
    "    - Ensuring response relevance\n",
    "\n",
    "5. **Error Handling and Robustness**  \n",
    "    - Managing retrieval and generation exceptions\n",
    "    - Providing clear user feedback\n",
    "    - Ensuring pipeline reliability\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "This notebook provides a foundational implementation of RAG techniques. For more advanced, practical examples, please refer langchain documentation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents loaded: 3\n",
      "Total document chunks: 64\n",
      "\n",
      "--- Retrieved Documents ---\n",
      "Total documents retrieved: 5\n",
      "--------------------------------------------------\n",
      "\n",
      "Document 1:\n",
      "Score: N/A\n",
      "Source: https://python.langchain.com/docs/introduction/\n",
      "\n",
      "Content:\n",
      "LangChain\n",
      " is a framework for developing applications powered by large language models (LLMs).\n",
      "\n",
      "\n",
      "LangChain simplifies every stage of the LLM application lifecycle:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Development\n",
      ": Build your applications using LangChain's open-source \n",
      "components\n",
      " and \n",
      "third-party integrations\n",
      ".\n",
      "Use \n",
      "LangGraph\n",
      " to build stateful agents with first-class streaming and human-in-the-loop support.\n",
      "\n",
      "\n",
      "Productionization\n",
      ": Use \n",
      "LangSmith\n",
      " to inspect, monitor and evaluate your applications, so that you can continuously o... [truncated]\n",
      "--------------------------------------------------\n",
      "\n",
      "Document 2:\n",
      "Score: N/A\n",
      "Source: https://python.langchain.com/docs/introduction/\n",
      "\n",
      "Content:\n",
      "Architecture\n",
      " page.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "langchain-core\n",
      ": Base abstractions for chat models and other components.\n",
      "\n",
      "\n",
      "Integration packages\n",
      " (e.g. \n",
      "langchain-openai\n",
      ", \n",
      "langchain-anthropic\n",
      ", etc.): Important integrations have been split into lightweight packages that are co-maintained by the LangChain team and the integration developers.\n",
      "\n",
      "\n",
      "langchain\n",
      ": Chains, agents, and retrieval strategies that make up an application's cognitive architecture.\n",
      "\n",
      "\n",
      "langchain-community\n",
      ": Third-party integrations that are community main... [truncated]\n",
      "--------------------------------------------------\n",
      "\n",
      "Document 3:\n",
      "Score: N/A\n",
      "Source: https://python.langchain.com/docs/concepts/chat_models/\n",
      "\n",
      "Content:\n",
      "LangChain supports two message formats to interact with chat models:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LangChain Message Format\n",
      ": LangChain's own message format, which is used by default and is used internally by LangChain.\n",
      "\n",
      "\n",
      "OpenAI's Message Format\n",
      ": OpenAI's message format.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Standard parameters\n",
      "â€‹\n",
      "\n",
      "\n",
      "Many chat models have standardized parameters that can be used to configure the model:\n",
      "--------------------------------------------------\n",
      "\n",
      "Document 4:\n",
      "Score: N/A\n",
      "Source: https://python.langchain.com/docs/introduction/\n",
      "\n",
      "Content:\n",
      "Introduction | ðŸ¦œï¸ðŸ”— LangChain\n",
      "--------------------------------------------------\n",
      "\n",
      "Document 5:\n",
      "Score: N/A\n",
      "Source: https://python.langchain.com/docs/introduction/\n",
      "\n",
      "Content:\n",
      "Embedding models\n",
      "Evaluation\n",
      "Example selectors\n",
      "Few-shot prompting\n",
      "Conceptual guide\n",
      "Key-value stores\n",
      "LangChain Expression Language (LCEL)\n",
      "Messages\n",
      "Multimodality\n",
      "Output parsers\n",
      "Prompt Templates\n",
      "Retrieval augmented generation (RAG)\n",
      "Retrieval\n",
      "Retrievers\n",
      "Runnable interface\n",
      "Streaming\n",
      "Structured outputs\n",
      "Testing\n",
      "String-in, string-out llms\n",
      "Text splitters\n",
      "Tokens\n",
      "Tool calling\n",
      "Tools\n",
      "Tracing\n",
      "Vector stores\n",
      "Why LangChain?\n",
      "Ecosystem\n",
      "ðŸ¦œðŸ› ï¸ LangSmith\n",
      "ðŸ¦œðŸ•¸ï¸ LangGraph\n",
      "Versions\n",
      "v0.3\n",
      "v0.2\n",
      "Pydantic compatibility\n",
      "Migrating ... [truncated]\n",
      "--------------------------------------------------\n",
      "Vector store saved to ../langchain_docs.\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import warnings\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.docstore.document import Document  # Import Document class\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "import faiss\n",
    "\n",
    "def load_html_documents(urls):\n",
    "    \"\"\"\n",
    "    Load HTML documents from a list of URLs.\n",
    "    \n",
    "    Args:\n",
    "        urls (list): List of URLs to fetch HTML content from\n",
    "    \n",
    "    Returns:\n",
    "        list: List of Document objects\n",
    "    \"\"\"\n",
    "    docs = []\n",
    "    \n",
    "    for url in urls:\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Extract text content from the HTML\n",
    "            text = soup.get_text(separator=\"\\n\")\n",
    "            # Convert to Document object\n",
    "            doc = Document(page_content=text, metadata={\"source\": url})\n",
    "            docs.append(doc)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load {url}: {e}\")\n",
    "    \n",
    "    return docs\n",
    "\n",
    "def chunk_documents(docs, chunk_size=1000, chunk_overlap=100):\n",
    "    \"\"\"\n",
    "    Split documents into smaller chunks.\n",
    "    \n",
    "    Args:\n",
    "        docs (list): List of Document objects to chunk\n",
    "        chunk_size (int): Size of each document chunk\n",
    "        chunk_overlap (int): Overlap between chunks\n",
    "    \n",
    "    Returns:\n",
    "        list: List of Document chunks\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, \n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    return text_splitter.split_documents(docs)\n",
    "\n",
    "def create_vector_store(chunks, embedding_model='nomic-embed-text', base_url='http://localhost:11434'):\n",
    "    \"\"\"\n",
    "    Create a vector store from document chunks.\n",
    "    \n",
    "    Args:\n",
    "        chunks (list): List of document chunks\n",
    "        embedding_model (str): Name of the embedding model\n",
    "        base_url (str): Base URL for Ollama embeddings\n",
    "    \n",
    "    Returns:\n",
    "        FAISS: Vector store with embedded documents\n",
    "    \"\"\"\n",
    "    # Initialize embeddings\n",
    "    embeddings = OllamaEmbeddings(model=embedding_model, base_url=base_url)\n",
    "    \n",
    "    # Create vector embedding\n",
    "    vector = embeddings.embed_query(\"Sample Text\")\n",
    "    \n",
    "    # Create FAISS index\n",
    "    index = faiss.IndexFlatL2(len(vector))\n",
    "    vector_store = FAISS(\n",
    "        embedding_function=embeddings,\n",
    "        index=index,\n",
    "        docstore=InMemoryDocstore(),\n",
    "        index_to_docstore_id={},\n",
    "    )\n",
    "    \n",
    "    # Add documents to vector store\n",
    "    vector_store.add_documents(documents=chunks)\n",
    "    \n",
    "    return vector_store\n",
    "\n",
    "def print_retrieved_docs(retrieved_docs, max_length=500):\n",
    "    \"\"\"\n",
    "    Print retrieved documents in a clean, readable format.\n",
    "    \n",
    "    Args:\n",
    "        retrieved_docs (list): List of retrieved documents\n",
    "        max_length (int): Maximum length of content to display\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Retrieved Documents ---\")\n",
    "    print(f\"Total documents retrieved: {len(retrieved_docs)}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for i, doc in enumerate(retrieved_docs, 1):\n",
    "        print(f\"\\nDocument {i}:\")\n",
    "        print(f\"Score: {doc.metadata.get('score', 'N/A')}\")\n",
    "        print(f\"Source: {doc.metadata.get('source', 'Unknown')}\")\n",
    "        \n",
    "        # Truncate content if it's too long\n",
    "        content = doc.page_content\n",
    "        if len(content) > max_length:\n",
    "            content = content[:max_length] + \"... [truncated]\"\n",
    "        \n",
    "        print(\"\\nContent:\")\n",
    "        print(content)\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Main function to orchestrate document processing and vector store creation.\n",
    "    \"\"\"\n",
    "    # Suppress warnings (optional)\n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "    # List of URLs to process\n",
    "    urls = [\n",
    "        \"https://python.langchain.com/docs/introduction/\",\n",
    "        \"https://python.langchain.com/docs/concepts/chat_models/\",\n",
    "        \"https://python.langchain.com/docs/concepts/vectorstores/\"\n",
    "    ]\n",
    "    \n",
    "    # Load HTML documents\n",
    "    docs = load_html_documents(urls)\n",
    "    print(f\"Total documents loaded: {len(docs)}\")\n",
    "    \n",
    "    # Chunk documents\n",
    "    chunks = chunk_documents(docs)\n",
    "    print(f\"Total document chunks: {len(chunks)}\")\n",
    "    \n",
    "    # Create vector store\n",
    "    vector_store = create_vector_store(chunks)\n",
    "    \n",
    "    # Example retrieval\n",
    "    question = \"Which usecases  LangChain is used for?\"\n",
    "    try:\n",
    "        retrieved_docs = vector_store.similarity_search(query=question, k=5)\n",
    "        if not retrieved_docs:\n",
    "            print(\"No documents retrieved.\")\n",
    "        else:\n",
    "            print_retrieved_docs(retrieved_docs)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during similarity search: {e}\")\n",
    "        \n",
    "    # Optional: Save vector store\n",
    "    db_name = \"../langchain_docs\"\n",
    "    try:\n",
    "        vector_store.save_local(db_name)\n",
    "        print(f\"Vector store saved to {db_name}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save vector store: {e}\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store loaded from ../langchain_docs.\n",
      "\n",
      "--- Query ---\n",
      "Question: Which use cases is LangChain used for?\n",
      "\n",
      "--- Generated Answer ---\n",
      "Based on the provided context, LangChain appears to be primarily used for developing applications powered by large language models (LLMs). It simplifies various stages of the LLM application lifecycle, including development, productionization, and deployment. Additionally, LangChain supports building stateful agents with first-class streaming and human-in-the-loop support through its use of LangGraph.\n",
      "\n",
      "However, it's worth noting that the provided context does not explicitly mention specific use cases or applications for which LangChain is being used. Therefore, I will provide a general answer based on the information available:\n",
      "\n",
      "LangChain seems to be primarily used for developing LLM-powered applications, including chatbots, language models, and other intelligent agents. Its components are designed to simplify various stages of application development, productionization, and deployment, making it an attractive choice for building complex applications that rely on large language models.\n",
      "\n",
      "If you're looking for more specific information on LangChain's use cases or applications, I recommend checking out the official documentation or reaching out to the LangChain community for more guidance.\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "def build_retrieval_chain(retriever, llm, template):\n",
    "    \"\"\"\n",
    "    Build a retrieval chain with context-aware responses.\n",
    "    \n",
    "    Args:\n",
    "        retriever (Retriever): The retriever for fetching relevant documents\n",
    "        llm (ChatOllama): The language model for generation\n",
    "        template (str): The prompt template for the LLM\n",
    "    \n",
    "    Returns:\n",
    "        Runnable: The retrieval chain\n",
    "    \"\"\"\n",
    "    # Initialize chat prompt from template\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    # Build the retrieval chain\n",
    "    retrieval_chain = (\n",
    "        {\n",
    "            \"context\": retriever,  # This will automatically call .get_relevant_documents()\n",
    "            \"question\": RunnablePassthrough()\n",
    "        }\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    return retrieval_chain\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Main function to orchestrate document processing, vector store creation, and RAG demonstration.\n",
    "    \"\"\"\n",
    "    # Define the prompt template\n",
    "    template = \"\"\"You are an expert in LangChain and its use cases. \n",
    "    Answer the question based strictly on the following context:\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    If the context does not provide sufficient information, clearly state that you cannot provide a comprehensive answer based on the available information.\"\"\"\n",
    "\n",
    "    # Load the vector store from the saved location\n",
    "    db_name = \"../langchain_docs\"\n",
    "    try:\n",
    "        # Provide the required embeddings argument and enable dangerous deserialization\n",
    "        embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "        vector_store = FAISS.load_local(db_name, embeddings=embeddings, allow_dangerous_deserialization=True)\n",
    "        print(f\"Vector store loaded from {db_name}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load vector store: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "    # Configure retriever with search parameters\n",
    "    retriever = vector_store.as_retriever(\n",
    "        search_type=\"similarity\",  # or \"mmr\" for maximum marginal relevance\n",
    "        search_kwargs={\"k\": 4}  # number of documents to retrieve\n",
    "    )\n",
    "\n",
    "    # Initialize the language model\n",
    "    ollama_model = ChatOllama(\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        model='llama3.2:1b',\n",
    "        temperature=0.5,\n",
    "        num_predict=512\n",
    "    )\n",
    "\n",
    "    # Build the retrieval chain\n",
    "    retrieval_chain = build_retrieval_chain(retriever, ollama_model, template)\n",
    "\n",
    "    # RAG-based retrieval and generation\n",
    "    question = \"Which use cases is LangChain used for?\"\n",
    "    try:\n",
    "        print(\"\\n--- Query ---\")\n",
    "        print(f\"Question: {question}\")\n",
    "        \n",
    "        # Directly invoke the retrieval chain\n",
    "        result = retrieval_chain.invoke(question)\n",
    "        \n",
    "        print(\"\\n--- Generated Answer ---\")\n",
    "        print(result)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during RAG process: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
